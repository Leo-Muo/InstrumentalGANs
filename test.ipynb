{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from librosa.display import specshow\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import pad\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "\n",
    "# Add the path to the cloned models/research/audioset directory\n",
    "# sys.path.append(os.path.join('/home/lordvirg/university/GANmaster/Generate_Your_Own_Music/models/research/audioset/vggish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for input and output\n",
    "VOCALS_DIR = \"dataset/vocals\"\n",
    "INSTRUMENTALS_DIR = \"dataset/instrumentals\"\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "\n",
    "# Create processed directories\n",
    "os.makedirs(os.path.join(PROCESSED_DIR, \"vocals\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DIR, \"instrumentals\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_to_melspectrogram(file_path, n_fft=2048, hop_length=512, n_mels=128, target_duration=30, sr=22050):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    target_length = int(target_duration * sr)  # Number of samples for 30 seconds\n",
    "\n",
    "    # Trim or pad the audio to the target length\n",
    "    if len(y) < target_length:\n",
    "        padding = target_length - len(y)\n",
    "        y = np.pad(y, (0, padding), mode='constant')\n",
    "    elif len(y) > target_length:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to decibel scale\n",
    "\n",
    "    # Ensure the output is 2D (1 channel)\n",
    "    return mel_spec_db\n",
    "\n",
    "\n",
    "def normalize_spectrogram(spec):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    spec_normalized = scaler.fit_transform(spec)\n",
    "    return spec_normalized\n",
    "\n",
    "def segment_spectrogram(spec, frame_length=128):\n",
    "    segments = []\n",
    "    for i in range(0, spec.shape[1] - frame_length + 1, frame_length // 2):  # Overlapping frames\n",
    "        segments.append(spec[:, i:i + frame_length])\n",
    "    return np.array(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Vocals\n",
    "vocal_files = [f for f in os.listdir(VOCALS_DIR) if f.endswith(\".mp3\")]\n",
    "for file in vocal_files:\n",
    "    file_path = os.path.join(VOCALS_DIR, file)\n",
    "    mel_spec = mp3_to_melspectrogram(file_path)\n",
    "    mel_spec_normalized = normalize_spectrogram(mel_spec)\n",
    "    mel_segments = segment_spectrogram(mel_spec_normalized)\n",
    "    np.save(os.path.join(PROCESSED_DIR, \"vocals\", file.replace(\".mp3\", \"_mel_segments.npy\")), mel_segments)\n",
    "\n",
    "# Process Instrumentals\n",
    "instrumental_files = [f for f in os.listdir(INSTRUMENTALS_DIR) if f.endswith(\".mp3\")]\n",
    "for file in instrumental_files:\n",
    "    file_path = os.path.join(INSTRUMENTALS_DIR, file)\n",
    "    mel_spec = mp3_to_melspectrogram(file_path)\n",
    "    mel_spec_normalized = normalize_spectrogram(mel_spec)\n",
    "    mel_segments = segment_spectrogram(mel_spec_normalized)\n",
    "    np.save(os.path.join(PROCESSED_DIR, \"instrumentals\", file.replace(\".mp3\", \"_mel_segments.npy\")), mel_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalInstrumentalDataset(Dataset):\n",
    "    def __init__(self, vocal_dir, instrumental_dir):\n",
    "        self.vocal_files = sorted([os.path.join(vocal_dir, f) for f in os.listdir(vocal_dir) if f.endswith(\".npy\")])\n",
    "        self.instrumental_files = sorted([os.path.join(instrumental_dir, f) for f in os.listdir(instrumental_dir) if f.endswith(\".npy\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocal_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vocal = np.load(self.vocal_files[idx])  # Shape: [n_segments, n_mels, n_frames]\n",
    "        instrumental = np.load(self.instrumental_files[idx])\n",
    "\n",
    "        # Ensure single segment is used for each batch item\n",
    "        if len(vocal.shape) == 3:  # [n_segments, n_mels, n_frames]\n",
    "            vocal = vocal[0]  # Pick first segment or handle multiple segments as needed\n",
    "        if len(instrumental.shape) == 3:\n",
    "            instrumental = instrumental[0]\n",
    "\n",
    "        # Add channel dimension\n",
    "        vocal = np.expand_dims(vocal, axis=0)  # Shape: [1, n_mels, n_frames]\n",
    "        instrumental = np.expand_dims(instrumental, axis=0)\n",
    "\n",
    "        return torch.tensor(vocal, dtype=torch.float32), torch.tensor(instrumental, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec1 = nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.relu(self.enc1(x))\n",
    "        e2 = self.relu(self.enc2(e1))\n",
    "        e3 = self.relu(self.enc3(e2))\n",
    "        e4 = self.relu(self.enc4(e3))\n",
    "\n",
    "        # Decoding\n",
    "        d4 = self.relu(self.dec4(e4))\n",
    "        d3 = self.relu(self.dec3(d4 + e3))  # Skip connection\n",
    "        d2 = self.relu(self.dec2(d3 + e2))  # Skip connection\n",
    "        d1 = self.tanh(self.dec1(d2 + e1))  # Skip connection\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)  # Output single-channel feature map\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def adversarial_loss(predictions, targets):\n",
    "    return nn.MSELoss()(predictions, targets)\n",
    "\n",
    "def reconstruction_loss(predicted, target):\n",
    "    return nn.L1Loss()(predicted, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(generator, discriminator, dataloader, g_optimizer, d_optimizer, epochs, device):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    g_loss_item = 0\n",
    "    d_loss_item = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (vocal, instrumental) in enumerate(dataloader):\n",
    "            vocal, instrumental = vocal.to(device), instrumental.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            real_data = torch.cat((vocal, instrumental), dim=1)\n",
    "            fake_data = torch.cat((vocal, generator(vocal)), dim=1)\n",
    "\n",
    "            real_loss = adversarial_loss(discriminator(real_data), torch.ones_like(discriminator(real_data)))\n",
    "            fake_loss = adversarial_loss(discriminator(fake_data), torch.zeros_like(discriminator(fake_data)))\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_data = generator(vocal)\n",
    "            g_loss_adv = adversarial_loss(discriminator(torch.cat((vocal, fake_data), dim=1)), torch.ones_like(discriminator(torch.cat((vocal, fake_data), dim=1))))\n",
    "            g_loss_rec = reconstruction_loss(fake_data, instrumental)\n",
    "            g_loss = g_loss_adv + g_loss_rec\n",
    "\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "                \n",
    "            g_loss_item = g_loss.item()\n",
    "            d_loss_item = d_loss.item()\n",
    "            \n",
    "                    \n",
    "    return d_loss_item, g_loss_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(generator, dataloader, device):\n",
    "    generator.eval()\n",
    "    snr_list, lsd_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vocal, instrumental in dataloader:\n",
    "            vocal, instrumental = vocal.to(device), instrumental.to(device)\n",
    "            predicted = generator(vocal)\n",
    "\n",
    "            # Calculate SNR\n",
    "            signal_power = torch.sum(instrumental**2)\n",
    "            noise_power = torch.sum((instrumental - predicted)**2)\n",
    "            snr = 10 * torch.log10(signal_power / noise_power)\n",
    "            snr_list.append(snr.item())\n",
    "\n",
    "            # Calculate LSD\n",
    "            instrumental_spec = librosa.amplitude_to_db(torch.squeeze(instrumental.cpu()).numpy(), ref=np.max)\n",
    "            predicted_spec = librosa.amplitude_to_db(torch.squeeze(predicted.cpu()).numpy(), ref=np.max)\n",
    "            lsd = np.mean(np.sqrt(np.mean((instrumental_spec - predicted_spec)**2, axis=-1)))\n",
    "            lsd_list.append(lsd)\n",
    "\n",
    "    avg_snr = np.mean(snr_list)\n",
    "    avg_lsd = np.mean(lsd_list)\n",
    "\n",
    "    print(f\"Validation Results - SNR: {avg_snr:.4f}, LSD: {avg_lsd:.4f}\")\n",
    "    \n",
    "    return avg_snr, avg_lsd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 4D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m     d_loss, g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     avg_snr, avg_lsd \u001b[38;5;241m=\u001b[39m validate(generator, val_loader, device)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m avg_snr \u001b[38;5;241m>\u001b[39m best_snr \u001b[38;5;129;01mand\u001b[39;00m avg_lsd \u001b[38;5;241m<\u001b[39m best_lsd:\n",
      "Cell \u001b[0;32mIn[86], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, dataloader, g_optimizer, d_optimizer, epochs, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m real_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((vocal, instrumental), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((vocal, generator(vocal)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mones_like(discriminator(real_data)))\n\u001b[1;32m     18\u001b[0m fake_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(discriminator(fake_data), torch\u001b[38;5;241m.\u001b[39mzeros_like(discriminator(fake_data)))\n\u001b[1;32m     19\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (real_loss \u001b[38;5;241m+\u001b[39m fake_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 19\u001b[0m, in \u001b[0;36mLSTMDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pass input through LSTM layer\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Take the output of the last time step\u001b[39;00m\n\u001b[1;32m     22\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:876\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m--> 876\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    877\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    878\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: LSTM: Expected input to be 2D or 3D, got 4D instead"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = VocalInstrumentalDataset(\n",
    "    vocal_dir=os.path.join(PROCESSED_DIR, \"vocals\"),\n",
    "    instrumental_dir=os.path.join(PROCESSED_DIR, \"instrumentals\")\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(input_channels=1, output_channels=1).to(device)\n",
    "discriminator = Discriminator(input_channels=2).to(device)\n",
    "# discriminator = PatchDiscriminator(input_channels=2).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimizers and learning rate schedulers \n",
    "# g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=20, gamma=0.5)\n",
    "# d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# # Faster initial learning with steeper decay\n",
    "# g_optimizer = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "# d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "# g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=10, gamma=0.3)\n",
    "# d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=10, gamma=0.3)\n",
    "\n",
    "# Slower, more stable learning\n",
    "# g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.7, 0.999))\n",
    "# d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.7, 0.999))\n",
    "# g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=30, gamma=0.7)\n",
    "# d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=30, gamma=0.7)\n",
    "\n",
    "# Different learning rates for generator and discriminator\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=15, gamma=0.5)\n",
    "d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "# Using cosine annealing instead of step scheduling\n",
    "# g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# g_scheduler = optim.lr_scheduler.CosineAnnealingLR(g_optimizer, T_max=50, eta_min=1e-6)\n",
    "# d_scheduler = optim.lr_scheduler.CosineAnnealingLR(d_optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "# Using RMSprop instead of Adam\n",
    "# g_optimizer = optim.RMSprop(generator.parameters(), lr=0.0002, alpha=0.99)\n",
    "# d_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.0002, alpha=0.99)\n",
    "# g_scheduler = optim.lr_scheduler.ExponentialLR(g_optimizer, gamma=0.97)\n",
    "# d_scheduler = optim.lr_scheduler.ExponentialLR(d_optimizer, gamma=0.97)\n",
    "\n",
    "# Using linear warm-up and reduce on plateau\n",
    "# g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "# d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "# g_scheduler = optim.lr_scheduler.ReduceLROnPlateau(g_optimizer, mode='min', factor=0.5, patience=5)\n",
    "# d_scheduler = optim.lr_scheduler.ReduceLROnPlateau(d_optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Multiple learning rate drops at specific milestones\n",
    "# g_optimizer = optim.Adam(generator.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "# d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "# g_scheduler = optim.lr_scheduler.MultiStepLR(g_optimizer, milestones=[10, 20, 30], gamma=0.5)\n",
    "# d_scheduler = optim.lr_scheduler.MultiStepLR(d_optimizer, milestones=[10, 20, 30], gamma=0.5)\n",
    "\n",
    "# Cyclic learning rate strategy\n",
    "# g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "# d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "# g_scheduler = optim.lr_scheduler.CyclicLR(g_optimizer, base_lr=0.0001, max_lr=0.001, step_size_up=2000)\n",
    "# d_scheduler = optim.lr_scheduler.CyclicLR(d_optimizer, base_lr=0.0001, max_lr=0.001, step_size_up=2000)\n",
    "\n",
    "best_snr = float('-inf')  # Track the best SNR value\n",
    "best_lsd = float('inf')   # Track the best LSD value\n",
    "best_epoch = 0          # Track the best epoch\n",
    "\n",
    "\n",
    "# Training and validation\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    d_loss, g_loss = train(generator, discriminator, train_loader, g_optimizer, d_optimizer, epochs=5, device=device)\n",
    "    avg_snr, avg_lsd = validate(generator, val_loader, device)\n",
    "    \n",
    "    if avg_snr > best_snr and avg_lsd < best_lsd:\n",
    "        best_snr = avg_snr\n",
    "        best_lsd = avg_lsd\n",
    "        best_epoch = epoch \n",
    "    \n",
    "    print(f\"Best Results - Epoch: {best_epoch}, Best SNR: {best_snr:.4f}, Best LSD: {best_lsd:.4f}\")\n",
    "    \n",
    "\n",
    "    # Step schedulers\n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "    \n",
    "    # g_scheduler.step(g_loss) \n",
    "    # d_scheduler.step(d_loss) \n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
