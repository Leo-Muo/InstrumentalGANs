{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from music21 import stream, note, chord, instrument\n",
    "# from create_generator_model import get_notes, LATENT_DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_midi(prediction_output, filename):\n",
    "    \"\"\"Convert the output from the prediction to notes and create a MIDI file from the notes.\"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # Define a list of instrument classes to choose from\n",
    "    instrument_classes = [\n",
    "    instrument.Accordion,\n",
    "    instrument.AcousticBass,\n",
    "    instrument.AcousticGuitar,\n",
    "    instrument.Agogo,\n",
    "    instrument.Alto,\n",
    "    instrument.AltoSaxophone,\n",
    "    instrument.Bagpipes,\n",
    "    instrument.Banjo,\n",
    "    instrument.Baritone,\n",
    "    instrument.BaritoneSaxophone,\n",
    "    instrument.Bass,\n",
    "    instrument.BassClarinet,\n",
    "    instrument.BassDrum,\n",
    "    instrument.BassTrombone,\n",
    "    instrument.Bassoon,\n",
    "    instrument.BongoDrums,\n",
    "    instrument.BrassInstrument,\n",
    "    instrument.Castanets,\n",
    "    instrument.Celesta,\n",
    "    instrument.Choir,\n",
    "    instrument.Clarinet,\n",
    "    instrument.Clavichord,\n",
    "    instrument.Conductor,\n",
    "    instrument.CongaDrum,\n",
    "    instrument.Contrabass,\n",
    "    instrument.Contrabassoon,\n",
    "    instrument.Cowbell,\n",
    "    instrument.CrashCymbals,\n",
    "    instrument.Cymbals,\n",
    "    instrument.Dulcimer,\n",
    "    instrument.ElectricBass,\n",
    "    instrument.ElectricGuitar,\n",
    "    instrument.ElectricOrgan,\n",
    "    instrument.ElectricPiano,\n",
    "    instrument.EnglishHorn,\n",
    "    instrument.FingerCymbals,\n",
    "    instrument.Flute,\n",
    "    instrument.FretlessBass,\n",
    "    instrument.Glockenspiel,\n",
    "    instrument.Gong,\n",
    "    instrument.Guitar\n",
    "]\n",
    "\n",
    "    # Track the last instrument used to avoid redundant instrument changes\n",
    "    last_instrument = None\n",
    "\n",
    "    # Create note and chord objects based on the values generated by the model\n",
    "    for item in prediction_output:\n",
    "        pattern = item[0]\n",
    "        # Choose an instrument class for this note or chord\n",
    "        instr_class = random.choice(instrument_classes)\n",
    "        print(instr_class)\n",
    "        instr = instr_class()\n",
    "\n",
    "        # Add the instrument change only if it is different from the last instrument\n",
    "        if type(last_instrument) != type(instr):\n",
    "            output_notes.append(instr)\n",
    "            last_instrument = instr\n",
    "\n",
    "        # Pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # Pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # Increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='{}.mid'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(generator_model, latent_dim, n_vocab, length=500):\n",
    "    \"\"\" Generate new music using the trained generator model \"\"\"\n",
    "    # Create random noise as input to the generator\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    predictions = generator_model.predict(noise)\n",
    "    \n",
    "    # Scale back the predictions to the original range\n",
    "    pred_notes = [x * (n_vocab / 2) + (n_vocab / 2) for x in predictions[0]]\n",
    "    \n",
    "    # Map generated integer indices to note names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    pred_notes_mapped = [int_to_note[int(x)] for x in pred_notes]\n",
    "    \n",
    "    return pred_notes_mapped[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, stream\n",
    "from pathlib import Path\n",
    "\n",
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    for file in Path(\"midi\").glob(\"*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIMENSION = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi/instr_6.mid\n",
      "Parsing midi/instr_30.mid\n",
      "Parsing midi/instr_33.mid\n",
      "Parsing midi/instr_51.mid\n",
      "Parsing midi/instr_27.mid\n",
      "Parsing midi/instr_36.mid\n",
      "Parsing midi/instr_10.mid\n",
      "Parsing midi/instr_23.mid\n",
      "Parsing midi/instr_8.mid\n",
      "Parsing midi/instr_22.mid\n",
      "Parsing midi/instr_15.mid\n",
      "Parsing midi/instr_50.mid\n",
      "Parsing midi/instr_38.mid\n",
      "Parsing midi/instr_18.mid\n",
      "Parsing midi/instr_24.mid\n",
      "Parsing midi/instr_45.mid\n",
      "Parsing midi/instr_34.mid\n",
      "Parsing midi/instr_11.mid\n",
      "Parsing midi/instr_44.mid\n",
      "Parsing midi/instr_17.mid\n",
      "Parsing midi/instr_43.mid\n",
      "Parsing midi/instr_16.mid\n",
      "Parsing midi/instr_39.mid\n",
      "Parsing midi/instr_7.mid\n",
      "Parsing midi/instr_48.mid\n",
      "Parsing midi/instr_5.mid\n",
      "Parsing midi/instr_2.mid\n",
      "Parsing midi/instr_31.mid\n",
      "Parsing midi/instr_14.mid\n",
      "Parsing midi/instr_35.mid\n",
      "Parsing midi/instr_25.mid\n",
      "Parsing midi/instr_46.mid\n",
      "Parsing midi/instr_47.mid\n",
      "Parsing midi/instr_32.mid\n",
      "Parsing midi/instr_26.mid\n",
      "Parsing midi/instr_13.mid\n",
      "Parsing midi/instr_42.mid\n",
      "Parsing midi/instr_3.mid\n",
      "Parsing midi/instr_21.mid\n",
      "Parsing midi/instr_41.mid\n",
      "Parsing midi/instr_1.mid\n",
      "Parsing midi/instr_4.mid\n",
      "Parsing midi/instr_9.mid\n",
      "Parsing midi/instr_20.mid\n",
      "Parsing midi/instr_28.mid\n",
      "Parsing midi/instr_40.mid\n",
      "Parsing midi/instr_29.mid\n",
      "Parsing midi/instr_12.mid\n",
      "Parsing midi/instr_37.mid\n",
      "Parsing midi/instr_19.mid\n",
      "Parsing midi/instr_49.mid\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f68e07276d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f68e07276d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Violin'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Flute'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Bass'>\n",
      "<class 'music21.instrument.Guitar'>\n",
      "<class 'music21.instrument.Piano'>\n",
      "<class 'music21.instrument.Bass'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206486/3517239568.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_notes_mapped = [int_to_note[int(x)] for x in pred_notes]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained generator model\n",
    "generator_model = load_model(\"conditional_music_gan_generator.h5\")\n",
    "\n",
    "# Load the processed notes and get the number of unique pitches\n",
    "notes = get_notes()\n",
    "n_vocab = len(set(notes))\n",
    "\n",
    "# Generate new music sequence\n",
    "generated_music = generate_music(generator_model, LATENT_DIMENSION, n_vocab)\n",
    "\n",
    "# Create a MIDI file from the generated music\n",
    "create_midi(generated_music, 'generated_music')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:54:48.834955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-27 22:54:50.265970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_notes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditional_music_gan_generator.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     85\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Example latent dimension size\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m n_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mget_notes\u001b[49m())  \u001b[38;5;66;03m# Number of unique notes in your dataset\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the generator model\u001b[39;00m\n\u001b[1;32m     89\u001b[0m generator_model \u001b[38;5;241m=\u001b[39m load_generator_model(model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_notes' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from music21 import stream, note, chord, instrument\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to load the generator model\n",
    "def load_generator_model(model_path):\n",
    "    return load_model(model_path)\n",
    "\n",
    "# Function to generate music using the model's predictions\n",
    "def generate_music(generator_model, latent_dim, n_vocab, length=500):\n",
    "    \"\"\"\n",
    "    Generate new music using the trained generator model.\n",
    "    - generator_model: The trained model used for prediction.\n",
    "    - latent_dim: The input dimension of the latent space for the generator.\n",
    "    - n_vocab: The number of unique notes in the dataset.\n",
    "    - length: The length of the generated music sequence.\n",
    "    \"\"\"\n",
    "    # Create random noise as input to the generator\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    \n",
    "    # Predict using the generator model\n",
    "    predictions = generator_model.predict(noise)\n",
    "    \n",
    "    # Scale back the predictions to the original range of notes\n",
    "    pred_notes = [x * (n_vocab / 2) + (n_vocab / 2) for x in predictions[0]]\n",
    "\n",
    "    # Map generated integer indices to note names\n",
    "    pitchnames = sorted(set(item for item in get_notes()))  # get_notes should return all available notes\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    pred_notes_mapped = [int_to_note[int(x)] for x in pred_notes]\n",
    "\n",
    "    return pred_notes_mapped[:length]\n",
    "\n",
    "# Function to create MIDI from generated music\n",
    "def create_midi(prediction_output, filename):\n",
    "    \"\"\"Convert the output from the prediction to notes and create a MIDI file.\"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # Define a list of instrument classes to choose from\n",
    "    instrument_classes = [\n",
    "        instrument.AcousticGuitar, instrument.Piano, instrument.Flute, \n",
    "        instrument.Bass, instrument.Violin, instrument.Guitar\n",
    "    ]\n",
    "\n",
    "    # Track the last instrument used to avoid redundant instrument changes\n",
    "    last_instrument = None\n",
    "\n",
    "    for pattern in prediction_output:\n",
    "        # Choose an instrument class for this note or chord\n",
    "        instr_class = random.choice(instrument_classes)\n",
    "        instr = instr_class()\n",
    "\n",
    "        # Add the instrument change only if it is different from the last instrument\n",
    "        if type(last_instrument) != type(instr):\n",
    "            output_notes.append(instr)\n",
    "            last_instrument = instr\n",
    "\n",
    "        # If pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # If pattern is a single note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # Increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
    "\n",
    "# Usage example\n",
    "model_path = 'conditional_music_gan_generator.h5'\n",
    "latent_dim = 1000  # Example latent dimension size\n",
    "n_vocab = len(get_notes())  # Number of unique notes in your dataset\n",
    "\n",
    "# Load the generator model\n",
    "generator_model = load_generator_model(model_path)\n",
    "\n",
    "# Generate music\n",
    "generated_notes = generate_music(generator_model, latent_dim, n_vocab)\n",
    "\n",
    "# Create a MIDI file from the generated notes\n",
    "create_midi(generated_notes, 'generated_music')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
