{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 15:44:32.191604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-19 15:44:33.273568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional, Flatten, Concatenate, BatchNormalization, LeakyReLU, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import utils  \n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(path_dataset):\n",
    "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    for file in Path(path_dataset).glob(\"*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab, sequence_length=10):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    pitch_names = sorted(set(notes))\n",
    "    \n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitch_names))\n",
    "    # Create input sequences and corresponding outputs\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    # print(notes)\n",
    "    # Create input sequences and the one-hot encoded output sequence\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        \n",
    "        # Convert input sequence to integer representation\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        \n",
    "        # Convert output note to integer\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "    \n",
    "    # Reshape input to be compatible with LSTM layers\n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    # Normalize input\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    # One-hot encode the output\n",
    "    network_output = utils.to_categorical(network_output)\n",
    "    \n",
    "    return network_input, network_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_midi(prediction_output, filename):\n",
    "#     \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "#         from the notes \"\"\"\n",
    "#     offset = 0\n",
    "#     output_notes = []\n",
    "\n",
    "#     # create note and chord objects based on the values generated by the model\n",
    "#     for item in prediction_output:\n",
    "#         pattern = item[0]\n",
    "#         # pattern is a chord\n",
    "#         if ('.' in pattern) or pattern.isdigit():\n",
    "#             notes_in_chord = pattern.split('.')\n",
    "#             notes = []\n",
    "#             for current_note in notes_in_chord:\n",
    "#                 new_note = note.Note(int(current_note))\n",
    "#                 new_note.storedInstrument = instrument.Piano()\n",
    "#                 notes.append(new_note)\n",
    "#             new_chord = chord.Chord(notes)\n",
    "#             new_chord.offset = offset\n",
    "#             output_notes.append(new_chord)\n",
    "#         # pattern is a note\n",
    "#         else:\n",
    "#             new_note = note.Note(pattern)\n",
    "#             new_note.offset = offset\n",
    "#             new_note.storedInstrument = instrument.Piano()\n",
    "#             output_notes.append(new_note)\n",
    "\n",
    "#         # increase offset each iteration so that notes do not stack\n",
    "#         offset += 0.5\n",
    "\n",
    "#     midi_stream = stream.Stream(output_notes)\n",
    "#     midi_stream.write('midi', fp='{}.mid'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalMusicGAN:\n",
    "    def __init__(self, rows, acapella_feature_dim=128):\n",
    "        self.seq_length = rows\n",
    "        self.seq_shape = (self.seq_length, 1)\n",
    "        self.latent_dim = 1000\n",
    "        self.acapella_feature_dim = acapella_feature_dim\n",
    "        \n",
    "        self.disc_loss = []\n",
    "        self.gen_loss = []\n",
    "        \n",
    "        # Optimizers with different learning rates\n",
    "        d_lr, d_beta_1 = 2.5932849036781864e-05, 0.467\n",
    "        g_lr, g_beta_1 = 3.2136735964331895e-05, 0.390\n",
    "        \n",
    "        self.d_optimizer = Adam(d_lr, d_beta_1)\n",
    "        self.g_optimizer = Adam(g_lr, g_beta_1)\n",
    "\n",
    "        # Build condition embedding\n",
    "        self.condition_input, self.condition_embedding = self.build_condition_embedding()\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=self.d_optimizer, \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Combined model for training generator\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        condition_z = Input(shape=(self.acapella_feature_dim,))\n",
    "        \n",
    "        self.discriminator.trainable = False\n",
    "        generated_seq = self.generator([z, condition_z])\n",
    "        validity = self.discriminator([generated_seq, condition_z])\n",
    "\n",
    "        self.combined = Model([z, condition_z], validity)\n",
    "        self.combined.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=self.g_optimizer\n",
    "        )\n",
    "\n",
    "    def build_condition_embedding(self):\n",
    "        condition_input = Input(shape=(self.acapella_feature_dim,))\n",
    "        condition_embedding = Dense(128, activation='relu')(condition_input)\n",
    "        return condition_input, condition_embedding\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        seq_input = Input(shape=self.seq_shape)\n",
    "        condition_input = Input(shape=(self.acapella_feature_dim,))\n",
    "        condition_embedding = Dense(128, activation='relu')(condition_input)\n",
    "        \n",
    "        x = Concatenate()([Flatten()(seq_input), condition_embedding])\n",
    "        x = Dense(512)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dense(256)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dense(100)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        validity = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        return Model([seq_input, condition_input], validity)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise_input = Input(shape=(self.latent_dim,))\n",
    "        condition_input = Input(shape=(self.acapella_feature_dim,))\n",
    "        x = Concatenate()([noise_input, condition_input])\n",
    "        x = Dense(256)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Dense(512)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Dense(1024)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Dense(np.prod(self.seq_shape), activation='tanh')(x)\n",
    "        seq = Reshape(self.seq_shape)(x)\n",
    "        \n",
    "        return Model([noise_input, condition_input], seq)\n",
    "\n",
    "    def extract_acapella_features(self, audio_path):\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(audio_path)\n",
    "        \n",
    "        # Extract 128 MFCCs (instead of 13)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=128)\n",
    "        \n",
    "        # Extract additional spectral features (can also increase dimensionality here if needed)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        \n",
    "        # Flatten the features to a 1D vector and ensure consistent dimensionality\n",
    "        mfcc_means = np.mean(mfccs, axis=1)  # Average MFCC coefficients across time\n",
    "        spectral_centroid_mean = np.mean(spectral_centroids)\n",
    "        \n",
    "        # Concatenate features to make a single vector\n",
    "        features = np.concatenate([mfcc_means, [spectral_centroid_mean]])\n",
    "        \n",
    "        # Normalize the features (optional)\n",
    "        features = (features - np.mean(features)) / np.std(features)\n",
    "        \n",
    "        # Ensure the features have exactly 128 elements (you can add more features or pad if necessary)\n",
    "        while len(features) < 128:\n",
    "            features = np.concatenate([features, np.zeros(128 - len(features))])\n",
    "        \n",
    "        # If the features exceed 128, you can trim them\n",
    "        features = features[:128]\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def train(self, epochs, acapella_directory, batch_size=128, sample_interval=50):\n",
    "        # Load acapella features\n",
    "        acapella_paths = list(Path(acapella_directory).glob('*.wav'))\n",
    "        \n",
    "        if not acapella_paths:\n",
    "            raise ValueError(\"No acapella files found in the specified directory\")\n",
    "    \n",
    "        # Load music data (you'll need to implement these functions)\n",
    "        notes = get_notes(\"dataset/instrumental\")\n",
    "        n_vocab = len(set(notes))\n",
    "        X_train, y_train = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            current_acapella_path = np.random.choice(acapella_paths)\n",
    "            acapella_features = self.extract_acapella_features(current_acapella_path)\n",
    "            \n",
    "            # Expand features to batch size\n",
    "            acapella_features_batch = np.tile(acapella_features, (batch_size, 1))\n",
    "        \n",
    "            # Train Discriminator\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_seqs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_seqs = self.generator.predict([noise, acapella_features_batch])\n",
    "\n",
    "            # Discriminator training\n",
    "            d_loss_real = self.discriminator.train_on_batch(\n",
    "                [real_seqs, acapella_features_batch], real\n",
    "            )\n",
    "            d_loss_fake = self.discriminator.train_on_batch(\n",
    "                [gen_seqs, acapella_features_batch], fake\n",
    "            )\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.combined.train_on_batch(\n",
    "                [noise, acapella_features_batch], real\n",
    "            )\n",
    "\n",
    "            # Logging\n",
    "            if epoch % sample_interval == 0:\n",
    "                print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}%] [G loss: {g_loss}]\")\n",
    "                self.disc_loss.append(d_loss[0])\n",
    "                self.gen_loss.append(g_loss)\n",
    "\n",
    "        # Save the generator model to an .h5 file after training\n",
    "        self.generator.save('conditional_music_gan_generator.h5')\n",
    "        print(\"Generator model saved as 'conditional_music_gan_generator.h5'\")\n",
    "\n",
    "        # Optionally save the discriminator as well\n",
    "        self.discriminator.save('conditional_music_gan_discriminator.h5')\n",
    "        print(\"Discriminator model saved as 'conditional_music_gan_discriminator.h5'\")\n",
    "\n",
    "        # Return the saved models if needed\n",
    "        return self.generator, self.discriminator\n",
    "\n",
    "    def generate(self, input_notes, acapella_features, filename='generated_music'):\n",
    "        pitchnames = sorted(set(input_notes))\n",
    "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
    "        condition_input = np.tile(acapella_features, (1, 1))\n",
    "        \n",
    "        # Generate sequence using the trained generator\n",
    "        predictions = self.generator.predict([noise, condition_input])\n",
    "        \n",
    "        # Convert the generated sequence into music notes\n",
    "        pred_notes = [x*242+242 for x in predictions[0]]\n",
    "        \n",
    "        pred_notes_mapped = []\n",
    "        for x in pred_notes:\n",
    "            index = int(x)\n",
    "            if index in int_to_note:\n",
    "                pred_notes_mapped.append(int_to_note[index])\n",
    "            else:\n",
    "                pred_notes_mapped.append('C5')\n",
    "\n",
    "        # Create the MIDI file\n",
    "        self.create_midi(pred_notes_mapped, filename)\n",
    "\n",
    "    def create_midi(self, prediction_output, filename):\n",
    "        offset = 0\n",
    "        output_notes = []\n",
    "\n",
    "        # Define a list of instrument classes to choose from\n",
    "        instrument_classes = [\n",
    "            instrument.AcousticBass, instrument.AcousticGuitar,instrument.Alto, instrument.Baritone, instrument.Bass,\n",
    "            instrument.BassDrum, instrument.BassTrombone, instrument.BrassInstrument,\n",
    "            instrument.Choir, instrument.Clarinet, instrument.Contrabass,\n",
    "            instrument.ElectricBass, instrument.ElectricGuitar, instrument.ElectricOrgan, instrument.ElectricPiano,\n",
    "            instrument.Flute, instrument.FretlessBass, instrument.Guitar\n",
    "        ]\n",
    "\n",
    "        # Track the last instrument used to avoid redundant instrument changes\n",
    "        last_instrument = None\n",
    "\n",
    "        # Create note and chord objects based on the values generated by the model\n",
    "        for pattern in prediction_output:\n",
    "            # Choose an instrument class for this note or chord\n",
    "            instr_class = random.choice(instrument_classes)\n",
    "            instr = instr_class()\n",
    "\n",
    "            # Add the instrument change only if it is different from the last instrument\n",
    "            if type(last_instrument) != type(instr):\n",
    "                output_notes.append(instr)\n",
    "                last_instrument = instr\n",
    "\n",
    "            # Pattern is a chord\n",
    "            if ('.' in pattern) or pattern.isdigit():\n",
    "                notes_in_chord = pattern.split('.')\n",
    "                notes = []\n",
    "                for current_note in notes_in_chord:\n",
    "                    new_note = note.Note(int(current_note))\n",
    "                    notes.append(new_note)\n",
    "                new_chord = chord.Chord(notes)\n",
    "                new_chord.offset = offset\n",
    "                output_notes.append(new_chord)\n",
    "            # Pattern is a note\n",
    "            else:\n",
    "                new_note = note.Note(pattern)\n",
    "                new_note.offset = offset\n",
    "                output_notes.append(new_note)\n",
    "\n",
    "            # Increase offset each iteration so that notes do not stack\n",
    "            offset += 0.5\n",
    "\n",
    "        midi_stream = stream.Stream(output_notes)\n",
    "        midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.disc_loss, c='red', label='Discriminator Loss')\n",
    "        plt.plot(self.gen_loss, c='blue', label='Generator Loss')\n",
    "        plt.title(\"Conditional GAN Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('Conditional_GAN_Loss.png', transparent=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run the Conditional GAN\u001b[39;00m\n\u001b[1;32m      8\u001b[0m gan \u001b[38;5;241m=\u001b[39m ConditionalMusicGAN(rows\u001b[38;5;241m=\u001b[39mSEQUENCE_LENGTH)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43macapella_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset/vocals\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_INTERVAL\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 123\u001b[0m, in \u001b[0;36mConditionalMusicGAN.train\u001b[0;34m(self, epochs, acapella_directory, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    121\u001b[0m notes \u001b[38;5;241m=\u001b[39m get_notes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/instrumental\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m n_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(notes))\n\u001b[0;32m--> 123\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m real \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    126\u001b[0m fake \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mprepare_sequences\u001b[0;34m(notes, n_vocab, sequence_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m network_input \u001b[38;5;241m=\u001b[39m network_input \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(n_vocab)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# One-hot encode the output\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m network_output \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m network_input, network_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/numerical_utils.py:86\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m num_classes:\n\u001b[0;32m---> 86\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, num_classes))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2698\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "SEQUENCE_LENGTH = 10\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "SAMPLE_INTERVAL = 1\n",
    "\n",
    "# Run the Conditional GAN\n",
    "gan = ConditionalMusicGAN(rows=SEQUENCE_LENGTH)\n",
    "gan.train(\n",
    "    epochs=EPOCHS, \n",
    "    acapella_directory='dataset/vocals',\n",
    "    batch_size=BATCH_SIZE, \n",
    "    sample_interval=SAMPLE_INTERVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate music after training\n",
    "# Example acapella path and features\n",
    "acapella_path = 'test.wav'  # Replace with an actual path\n",
    "acapella_features = gan.extract_acapella_features(acapella_path)\n",
    "\n",
    "# Generate music using the trained generator\n",
    "input_notes = get_notes_from_midi('AnyConv.com__test.mid')\n",
    "gan.generate(input_notes, acapella_features,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
