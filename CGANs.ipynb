{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from librosa.display import specshow\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import pad\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "\n",
    "# Add the path to the cloned models/research/audioset directory\n",
    "# sys.path.append(os.path.join('/home/lordvirg/university/GANmaster/Generate_Your_Own_Music/models/research/audioset/vggish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for input and output\n",
    "VOCALS_DIR = \"dataset/vocals\"\n",
    "INSTRUMENTALS_DIR = \"dataset/instrumentals\"\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "\n",
    "# Create processed directories\n",
    "os.makedirs(os.path.join(PROCESSED_DIR, \"vocals\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DIR, \"instrumentals\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_to_melspectrogram(file_path, n_fft=2048, hop_length=512, n_mels=128, target_duration=30, sr=22050):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    target_length = int(target_duration * sr)  # Number of samples for 30 seconds\n",
    "\n",
    "    # Trim or pad the audio to the target length\n",
    "    if len(y) < target_length:\n",
    "        padding = target_length - len(y)\n",
    "        y = np.pad(y, (0, padding), mode='constant')\n",
    "    elif len(y) > target_length:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to decibel scale\n",
    "\n",
    "    # Ensure the output is 2D (1 channel)\n",
    "    return mel_spec_db\n",
    "\n",
    "\n",
    "def normalize_spectrogram(spec):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    spec_normalized = scaler.fit_transform(spec)\n",
    "    return spec_normalized\n",
    "\n",
    "def segment_spectrogram(spec, frame_length=128):\n",
    "    segments = []\n",
    "    for i in range(0, spec.shape[1] - frame_length + 1, frame_length // 2):  # Overlapping frames\n",
    "        segments.append(spec[:, i:i + frame_length])\n",
    "    return np.array(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to process the dataset\n",
    "\n",
    "# # Process Vocals\n",
    "# vocal_files = [f for f in os.listdir(VOCALS_DIR) if f.endswith(\".mp3\")]\n",
    "# for file in vocal_files:\n",
    "#     file_path = os.path.join(VOCALS_DIR, file)\n",
    "#     mel_spec = mp3_to_melspectrogram(file_path)\n",
    "#     mel_spec_normalized = normalize_spectrogram(mel_spec)\n",
    "#     mel_segments = segment_spectrogram(mel_spec_normalized)\n",
    "#     np.save(os.path.join(PROCESSED_DIR, \"vocals\", file.replace(\".mp3\", \"_mel_segments.npy\")), mel_segments)\n",
    "\n",
    "# # Process Instrumentals\n",
    "# instrumental_files = [f for f in os.listdir(INSTRUMENTALS_DIR) if f.endswith(\".mp3\")]\n",
    "# for file in instrumental_files:\n",
    "#     file_path = os.path.join(INSTRUMENTALS_DIR, file)\n",
    "#     mel_spec = mp3_to_melspectrogram(file_path)\n",
    "#     mel_spec_normalized = normalize_spectrogram(mel_spec)\n",
    "#     mel_segments = segment_spectrogram(mel_spec_normalized)\n",
    "#     np.save(os.path.join(PROCESSED_DIR, \"instrumentals\", file.replace(\".mp3\", \"_mel_segments.npy\")), mel_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalInstrumentalDataset(Dataset):\n",
    "    def __init__(self, vocal_dir, instrumental_dir):\n",
    "        self.vocal_files = sorted([os.path.join(vocal_dir, f) for f in os.listdir(vocal_dir) if f.endswith(\".npy\")])\n",
    "        self.instrumental_files = sorted([os.path.join(instrumental_dir, f) for f in os.listdir(instrumental_dir) if f.endswith(\".npy\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocal_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vocal = np.load(self.vocal_files[idx])  # Shape: [n_segments, n_mels, n_frames]\n",
    "        instrumental = np.load(self.instrumental_files[idx])\n",
    "\n",
    "        # Ensure single segment is used for each batch item\n",
    "        if len(vocal.shape) == 3:  # [n_segments, n_mels, n_frames]\n",
    "            vocal = vocal[0]  # Pick first segment or handle multiple segments as needed\n",
    "        if len(instrumental.shape) == 3:\n",
    "            instrumental = instrumental[0]\n",
    "\n",
    "        # Add channel dimension\n",
    "        vocal = np.expand_dims(vocal, axis=0)  # Shape: [1, n_mels, n_frames]\n",
    "        instrumental = np.expand_dims(instrumental, axis=0)\n",
    "\n",
    "        return torch.tensor(vocal, dtype=torch.float32), torch.tensor(instrumental, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec1 = nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.relu(self.enc1(x))\n",
    "        e2 = self.relu(self.enc2(e1))\n",
    "        e3 = self.relu(self.enc3(e2))\n",
    "        e4 = self.relu(self.enc4(e3))\n",
    "\n",
    "        # Decoding\n",
    "        d4 = self.relu(self.dec4(e4))\n",
    "        d3 = self.relu(self.dec3(d4 + e3))  # Skip connection\n",
    "        d2 = self.relu(self.dec2(d3 + e2))  # Skip connection\n",
    "        d1 = self.tanh(self.dec1(d2 + e1))  # Skip connection\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorVAE(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, latent_dim=128):\n",
    "        super(GeneratorVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.enc4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Compute encoder output size dynamically\n",
    "        self.latent_dim = latent_dim\n",
    "        self.flattened_size = None  # Placeholder, set during forward pass\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = None  # Placeholder, initialized dynamically\n",
    "        self.fc_logvar = None\n",
    "        self.fc_latent = None\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec1 = nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def initialize_latent_layers(self, flattened_size):\n",
    "        self.fc_mu = nn.Linear(flattened_size, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(flattened_size, self.latent_dim)\n",
    "        self.fc_latent = nn.Linear(self.latent_dim, flattened_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar, device):\n",
    "        \"\"\"Reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std).to(device)  # Ensure tensor is on the correct device\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device  # Automatically get the device of the input tensor\n",
    "\n",
    "        # Encoding\n",
    "        e1 = self.relu(self.enc1(x))\n",
    "        e2 = self.relu(self.enc2(e1))\n",
    "        e3 = self.relu(self.enc3(e2))\n",
    "        e4 = self.relu(self.enc4(e3))\n",
    "\n",
    "        # Flatten and compute latent space\n",
    "        e4_flat = e4.view(e4.size(0), -1)\n",
    "\n",
    "        # Initialize latent layers dynamically if not already done\n",
    "        if self.flattened_size is None:\n",
    "            self.flattened_size = e4_flat.size(1)\n",
    "            self.initialize_latent_layers(self.flattened_size)\n",
    "\n",
    "        mu = self.fc_mu(e4_flat).to(device)\n",
    "        logvar = self.fc_logvar(e4_flat).to(device)\n",
    "        z = self.reparameterize(mu, logvar, device)\n",
    "\n",
    "        # Decode latent vector\n",
    "        z_unflat = self.fc_latent(z).view(e4.size(0), 512, 4, 4).to(device)\n",
    "        d4 = self.relu(self.dec4(z_unflat))\n",
    "        d3 = self.relu(self.dec3(d4 + e3))  # Skip connection\n",
    "        d2 = self.relu(self.dec2(d3 + e2))  # Skip connection\n",
    "        d1 = self.tanh(self.dec1(d2 + e1))  # Skip connection\n",
    "\n",
    "        return d1, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def adversarial_loss(predictions, targets):\n",
    "    return nn.MSELoss()(predictions, targets)\n",
    "\n",
    "def reconstruction_loss(predicted, target):\n",
    "    return nn.L1Loss()(predicted, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(generator, discriminator, dataloader, g_optimizer, d_optimizer, epochs, device):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    g_loss_item = 0\n",
    "    d_loss_item = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (vocal, instrumental) in enumerate(dataloader):\n",
    "            vocal, instrumental = vocal.to(device), instrumental.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            real_data = torch.cat((vocal, instrumental), dim=1)\n",
    "            fake_data = torch.cat((vocal, generator(vocal)), dim=1)\n",
    "\n",
    "            real_loss = adversarial_loss(discriminator(real_data), torch.ones_like(discriminator(real_data)))\n",
    "            fake_loss = adversarial_loss(discriminator(fake_data), torch.zeros_like(discriminator(fake_data)))\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_data = generator(vocal)\n",
    "            g_loss_adv = adversarial_loss(discriminator(torch.cat((vocal, fake_data), dim=1)), torch.ones_like(discriminator(torch.cat((vocal, fake_data), dim=1))))\n",
    "            g_loss_rec = reconstruction_loss(fake_data, instrumental)\n",
    "            g_loss = g_loss_adv + g_loss_rec\n",
    "\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "                \n",
    "            g_loss_item = g_loss.item()\n",
    "            d_loss_item = d_loss.item()\n",
    "            \n",
    "                    \n",
    "    return d_loss_item, g_loss_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(generator, dataloader, device):\n",
    "    generator.eval()\n",
    "    snr_list, lsd_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vocal, instrumental in dataloader:\n",
    "            vocal, instrumental = vocal.to(device), instrumental.to(device)\n",
    "            predicted = generator(vocal)\n",
    "\n",
    "            # Calculate SNR\n",
    "            signal_power = torch.sum(instrumental**2)\n",
    "            noise_power = torch.sum((instrumental - predicted)**2)\n",
    "            snr = 10 * torch.log10(signal_power / noise_power)\n",
    "            snr_list.append(snr.item())\n",
    "\n",
    "            # Calculate LSD\n",
    "            instrumental_spec = librosa.amplitude_to_db(torch.squeeze(instrumental.cpu()).numpy(), ref=np.max)\n",
    "            predicted_spec = librosa.amplitude_to_db(torch.squeeze(predicted.cpu()).numpy(), ref=np.max)\n",
    "            lsd = np.mean(np.sqrt(np.mean((instrumental_spec - predicted_spec)**2, axis=-1)))\n",
    "            lsd_list.append(lsd)\n",
    "\n",
    "    avg_snr = np.mean(snr_list)\n",
    "    avg_lsd = np.mean(lsd_list)\n",
    "\n",
    "    print(f\"Validation Results - SNR: {avg_snr:.4f}, LSD: {avg_lsd:.4f}\")\n",
    "    \n",
    "    return avg_snr, avg_lsd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = VocalInstrumentalDataset(\n",
    "    vocal_dir=os.path.join(PROCESSED_DIR, \"vocals\"),\n",
    "    instrumental_dir=os.path.join(PROCESSED_DIR, \"instrumentals\")\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(input_channels=1, output_channels=1).to(device)\n",
    "discriminator = Discriminator(input_channels=2).to(device)\n",
    "# generator = GeneratorVAE(input_channels=1, output_channels=1).to(device) # Code for VAE not working yet\n",
    "\n",
    "\n",
    "# Different learning rates for generator and discriminator - Best so far\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=15, gamma=0.5)\n",
    "d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "\n",
    "best_snr = float('-inf')  # Track the best SNR value\n",
    "best_lsd = float('inf')   # Track the best LSD value\n",
    "best_epoch = 0          # Track the best epoch\n",
    "\n",
    "\n",
    "# Training and validation\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    d_loss, g_loss = train(generator, discriminator, train_loader, g_optimizer, d_optimizer, epochs=5, device=device)\n",
    "    avg_snr, avg_lsd = validate(generator, val_loader, device)\n",
    "    \n",
    "    if avg_snr > best_snr and avg_lsd < best_lsd:\n",
    "        best_snr = avg_snr\n",
    "        best_lsd = avg_lsd\n",
    "        best_epoch = epoch \n",
    "    \n",
    "    print(f\"Best Results - Epoch: {best_epoch}, Best SNR: {best_snr:.4f}, Best LSD: {best_lsd:.4f}\")\n",
    "    \n",
    "\n",
    "    # Step schedulers\n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "    \n",
    "    # g_scheduler.step(g_loss) \n",
    "    # d_scheduler.step(d_loss) \n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
